\section{Implementering}
\label{sec:implementering}



%Overordnet består systemet av to delsystemer. Det ene delsystemet detekterer og sporer fugleaktivitet med et varmekamera, og det andre delsystemet henter inn værdata. Begge disse systemene sender data til en prosesseringsenhet, som så prosesserer dataen og sender det videre til en database, før det blir framstilt på en nettside. \todo{ganske gjentagende fra kap 3, mulig sløyfe}








%en referanse til Pi4? datablad elns?

\subsection{IR-kamera}\label{sec:impl:kamera}

\begin{figure}[!htbp]
\begin{minipage}[c]{0.58\textwidth}
Det infrarøde kameraet som benyttes er et FLIR C3-kamera.
Kameraet er håndholdt, men kan også koblet til en datamaskin, her Pi-en. 
Kameraet har en IR-sensor med synsvinkel på $41\degree\ \times\ 31\degree$, bilderate på 9 Hz, og kan detektere temperaturforskjeller på under $\SI{0.1}{\celsius}$. 
Kameraet vil kontinuerlig ta bilder med en valgbar frekvens, som optimaliseres for å gi best mulig deteksjon. 
Samtidig kan denne begrenses slik at bildebehandlingen ikke blir overbelastet. 
Bildene overføres til prosesseringsenheten via en USB-kabel som definert i systemkrav \idref{id:internoverføring}.
\end{minipage}
\hfill
\begin{minipage}[c]{0.38\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{implementering/c3.png}
    \caption{IR-kameraet som systemet bruker, en FLIR C3.}
    \label{fig:c3}
\end{minipage}
\end{figure}

\subsection{Prosesseringsenhet}\label{sec:impl:prosessor}

\begin{figure}[!htbp]
\begin{minipage}[c]{0.58\textwidth}
Til databehandling brukes en Raspberry Pi (heretter kalt Pi-en). 
Dette er en liten datamaskin på ett enkelt kretskort, men som tilbyr nok minne og prosesseringskraft til systemets formål. 
Dette tilfredsstiller systemkrav \idref{id:prosessor}. 
Pi-en utfører all bildebehandling samt prosessering av værdata og bruker trådløs kommunikasjon til overføring av data til databasen. 
Pi-en er modell 4B, med 4GB RAM og en 64-bits prosessor \cite{raspberry}. 
Operativsystemet som brukes er Raspbian Buster med desktop \cite{raspbian}, som er optimalisert for bruk på Pi-en.
\end{minipage}
\hfill
\begin{minipage}[c]{0.38\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{implementering/pi4.png}
    \caption{Prosesseringsenheten, en Raspberry Pi 4B.}
    \label{fig:pi}
\end{minipage}
\end{figure}


\subsection{Programvare}\label{sec:impl:programvare} 

Hovedprogrammet kjører på Pi-en og har som mål å analysere bildestrømmen fra kameraet og sende informasjon om antall fugler til en database.
Til bildebehandlingen brukes Python-rammeverket \textit{OpenCV} \cite{OpenCV}. 
\textit{OpenCV} er laget for å gi utviklere rask og enkel tilgang til avanserte algoritmer innen maskinlæring og datasyn.
Dette rammeverket kan dermed brukes til bilderprosessering i form av filtrering, deteksjon av objekter og sporing, kalt \textit{tracking}, i sanntid.

Den overordnede flyten i programmet er vist i \autoref{fig:hovedprogram}. 
Først kjører programmet gjennom en oppstartsprosedyre før det går inn i en løkke med sykliske hendelser.

\begin{figure}[!htbp]
\begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{implementering/Program/hovedprogram.pdf}
    \caption{Flytskjema for programmet.}
    \label{fig:hovedprogram}
\end{minipage}
\hfill
\begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{implementering/Program/oppstart.pdf}
    \caption{Flytskjema for oppsettet til programmet.}
    \label{fig:hovedprogram_oppstart}
\end{minipage}
\hfill
\begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{implementering/Program/main_loop.pdf}
    \caption{Flytskjema til hovedløkken.}
    \label{fig:hovedprogram_loop}
\end{minipage}
\end{figure}

%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.2\textwidth]{implementering/Program/hovedprogram.pdf}
%    \caption{Flytskjema for programmet.}
%    \label{fig:hovedprogram}
%\end{figure}

I oppstartsfasen initialiseres en loggfil og en link til kameraet, som vist i flytskjema i \autoref{fig:hovedprogram_oppstart}. Loggfilen er beskrevet i \autoref{sec:impl:programvare:logging}.

%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.3\textwidth]{implementering/Program/oppstart.pdf}
%    \caption{Flytskjema for oppsettet til programmet.}
%    \label{fig:hovedprogram_oppstart}
%\end{figure}

Programmet fortsetter så å kjøre gjennom en rekke sykliske hendelser for bildegjenkjenning og tracking. 
Dette krever at man tar i bruk flere av de avanserte algoritmene \textit{OpenCV} tilbyr.
Ytterligere informasjon om bildeprosessering i \autoref{sec:impl:programvare:bildepro}. 
Flyten til hovedløkken kan sees i \autoref{fig:hovedprogram_loop}.

%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.3\textwidth]{implementering/Program/main_loop.pdf}
%    \caption{Flytskjema til hovedløkken.}
%    \label{fig:hovedprogram_loop}
%\end{figure}

\textit{OpenCV} har en innbygd funksjon for å lese inn videostrømmer, slik at IR-kameraet kan benyttes som om det var et vanlig webkamera. 
Det er altså forholdsvis enkelt å lese en bildestrøm fra kameraet med Pi-en, og kode for dette er vist i \autoref{code:impl:programvare:bildestrøm}. 
Problemet blir dermed i hovedsak å prosessere og analysere disse bildene for å kunne gjenkjenne blobs og spore disse mellom bildene.

\begin{listing}[!htb]
\begin{minted}{python}
    vc = cv2.VideoCapture(0)        #starter bildestrøm fra kamera
    
    if vc.isOpened():               #sjekker om kameraet er åpnet
        rval, frame = vc.read()     #leser bilde fra kamera
    else:
        rval = False                #setter rval til False fordi kameraet ikke ble åpnet 
    
\end{minted}
\caption{Kodeeksempelet viser hvordan man kan bruke \textit{OpenCV} for å hente inn bilder fra en videostrøm.}
\label{code:impl:programvare:bildestrøm}
\end{listing}


\subsubsection{Bildeprosessering}\label{sec:impl:programvare:bildepro}

Det å få en datamaskin til å kjenne igjen et spesifikt objekt i et bilde er en komplisert problemstilling ingeniører har jobbet med i mange år.
\textit{OpenCV} tilbyr en god del verktøy som muliggjør nettopp dette.

Flyten for bildeprosesseringen er illustrert i \autoref{fig:bildeprosessering}. 

For å lettere og mer presist kunne analysere blobs må bildene filtreres. 
Dette er beskrevet i \autoref{sec:impl:programvare:filtrering}. 
Deretter kan man utføre \textit{blob-deteksjon} (se \autoref{sec:impl:programvare:blob-detection}) og tracking (se \autoref{sec:impl:programvare:tracking}). 
Dette gjøres ved at man først kjører trackingalgoritmen for å spore blobs fra tidligere bilder, før man kjører blob-deteksjon og starter tracking av nye blobs. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{implementering/Program/bildepros.pdf}%
    \caption{Flyten til bildeprosesseringen.}
    \label{fig:bildeprosessering}
\end{figure}


\subsubsection{Filtrering}\label{sec:impl:programvare:filtrering}


For å oppnå best mulig deteksjon av fugler, filtreres bildene for å redusere støy og annen unyttig informasjon. 
Ideelt sett vil fulgen og bakgrunnen på bildet ha helt forskjellige farger med tydelig konstrast, da det gjør det lettest å detektere fuglene. 
Filtreringsalgoritmen prøver å oppnå dette idealet. 

Først brukes den innebygde funksjonen i \textit{OpenCV}, \python{gaussian_blur()}, for redusere støy i bildet. 
Algoritmen som gjør hoveddelen av filteringen, heter Otsus metode \cite{otsu}. 
Algoritmen regner ut en terskelverdi for hvert bilde, som så kan brukes for å konvertere bildet om fra gråskala til sort-hvitt. 
Bilde blir gjort om til gråskala ved å bruke funksjonen \python{cvtColor()}, som er en innebygd funksjon i \textit{OpenCV}. Den gjør bilde om til gråskala ved å ta et vektet gjennomsnitt av fargeverdiene \cite{cvtColor}.


Otsus metode fungerer ved å først dele bildet inn i to klasser, bakgrunn og forgrunn. 
Deretter utregnes variansen i fargeverdier i og mellom begge klassene.
En terskelverdi velges så slik at variansen innad i klassen minimeres, og variansen mellom klassene maksimeres. 


Den matematiske sammenhengen i likning \eqref{Binarization} sier hvordan terskelverdien brukes til å forvandle bilde om til sort-hvitt. 
I den sammenhengen er $1$ svart og $0$ hvit. 
Funksjonen $f(x,y)$ gir fargeverdien til pikslene i gråskala og funksjonen $dst(x,y)$ gir fargeverdiene til bildet i sort-hvit. 
Variabelen $T$ er terskelverdien. 
For mer utdypning på hvordan algoritmen fungerer, se \cite{otsu}.
Implementasjonen av algoritmen  ligger i \textit{filters.py} under \textit{image\textunderscore processing} på GitHuben til Jolyu \cite{GitHub}.


\begin{align}
    dst(x,y)= \begin{cases} 
        1 \text{ hvis } f(x,y) > T\\
        0 \text{ ellers.}
   \end{cases}\label{Binarization}
\end{align}



Etter Otsus metode kan man så utføre \textit{morphology}-transformasjoner. 
Dette er funksjoner som er implementert i \textit{OpenCV}-rammeverket \cite{morph}. 
Her ønsker man å utføre \textit{erosion} og \textit{dilation}. 
Erosion brukes for å filtrere bort støy (white noise), men samtidig krymper det objektet innover fra kantene. 
Dette problemet løser \textit{OpenCV} ved dilation, som igjen øker størrelsen til objektet.
\textit{OpenCV} har implementerte funksjoner som bruker kombinasjoner av disse til å filtrere bort støy. 
\textit{Opening} brukes for å fjerne støy fra en bakgrunn, mens \textit{closing} brukes i det motsatte tilfellet for å fjerne små hull i et objekt (blobs) i forgrunnen. Dette gjøres ved opencv funksjonen \python{morphologyEx()}.
Disse kjøres etter Otsu-filteret for å filtrere bort eventuell støy fra de resulterende bildene.

%opencv dokumentasjon om morphology https://docs.opencv.org/trunk/d9/d61/tutorial_py_morphological_ops.html

\subsubsection{Blob-deteksjon}\label{sec:impl:programvare:blob-detection}

Rammeverket \textit{OpenCV} brukes også til blob-deteksjon. 
Funksjonen som brukes fra \textit{OpenCV} til dette, er \python{SimpleBlobDetector()}. 
Den returnerer et objekt av \python{detector}-klassen. 
Klassen inneholder alle parametere til blob-deteksjonen, det vil si hvilke krav som stilles til hva som regnes som en blob. 
I tillegg inneholder klassen en medlemsfunksjon, \python{detect}, som finner alle blobene i bildet og returnerer en liste med koordinatene til disse. 
Kodeeksempel \ref{code:impl:programvare:blob_detection_func} viser hvordan disse funksjonene er brukt. Implementasjonen av funksjonene og klassen finnes i GitHuben til \textit{OpenCV} \cite{OpencCV_Git}.
\begin{listing}[!htb]
\begin{minted}{python}
#blob-deteksjon parametere:
MAX_AREA = 5000
MIN_AREA = 10

def init_blob_detector():
    '''Setup SimpleBlobDetector parameters'''
    params = cv2.SimpleBlobDetector_Params()

    # Forandrer tersklene for areal
    params.maxArea = MAX_AREA
    params.minArea = MIN_AREA

    detector = cv2.SimpleBlobDetector_create(params)    #create detector

    return detector
    
def blob_detection(img):
    '''Function to detect blobs. Returns list of keypoints'''

    detector = init_blob_detector() |\label{code:impl:programvare:blob_detection_func:line:init_blob}|  #oppretter detektor med parametere for blobs
    keyPoints = detector.detect(img)  #analyserer og lager en liste med nøkkelpunkter
    
    return keyPoints
    
\end{minted}
\caption{Implementasjon av blob-deteksjon. Merk at koden her er noe redigert for å være mer lesevennlig. Blant annet brukes flere ulike parametere. }
\label{code:impl:programvare:blob_detection_func}
\end{listing}

Som vist i \autoref{code:impl:programvare:blob_detection_func}, oppretter \python{init_blob_detector()}-funksjonen i linje \ref{code:impl:programvare:blob_detection_func:line:init_blob} en \\\python{detector}, hvor det fritt kan endre på en rekke parametere, som for eksempel areal.     
Til slutt returnerer den en \python{detector} med parameterne satt til deres ønskede verdier. 
Funksjonen \\\python{blob_detection()} oppretter en slik \python{detector}ved å kalle på \python{init_blob_detector()}, og bruker den innebygde \python{detect}-funksjonen til \textit{OpenCV} til å analysere bildet. 

Den returnerer så en liste over \textit{nøkkelpunkter}, eller \textit{keypoints}. 
Et nøkkelpunkt er et objekt som inneholder koordinatene til en blob samt radiusen. 
Videre brukes \python{blob_detection()} hver gang nøkkelpunktene i et bilde skal bli funnet.

\subsubsection{Tracking}\label{sec:impl:programvare:tracking}
For å unngå at programmet teller samme fugl flere ganger, brukes tracking. 
Måten trackingen fungerer er at en tracker konstrueres og gis koordinater til et område på skjermen der det finnes et objekt. 
Dette objektet vil trackingen kjenne igjen i neste bilde, og den vil klare å følge objektet gjennom videostrømmen.
Siden det kan være flere fugler i bildet samtidig, må det være mulig å tracke alle uavhengig av hverandre. 
Dette gjøres ved å lage et \textit{multi-tracker} objekt som samler flere trackere. 
Hver av disse trackerene følger forskjellige fugler.
Ved bruk av flere trackere, er det viktig at hver fugl kun trackes en gang. 
Dette er for å unngå at den samme fuglen telles flere ganger. 
Dersom et objekt allerede har en tracker, vil multi-trackeren passe på at det ikke oprettes en ny tracker for samme objekt.


Når en fugl skal trackes er koordinatene til fuglen i bildet nødvendig. 
Dette hentes fra koden til blob-deteksjon. 
Blob-deteksjonskoden returnerer en liste med nøkkelpunkter. 
Disse inneholder senter og radius til blobene som er i bildet.
\textit{Trackerfunksjonen}, som skal tracke blobene, tar derimot inn en firkantet boks som må omslutte bloben helt.
For at trackingen skal fungere optimalt må boksen være en del større enn bloben.
Derfor er det behov for en funksjon som kan gjøre om nøkkelpunkter til bokser. 
Dette er vist i \autoref{code:keypointsToBoxes}. 
Funksjonen \python{KeypointsToBoxes(keypoint)} returnerer en liste som inneholder all informasjon om boksene som trengs til trackingen.

\begin{listing}[!htb]
\begin{minted}{python}
    def KeypointsToBoxes(keypoints):
        boxes = []  #liste for bokser rundt alle keypoints 
        for keypoint in keypoints:
        #boksene lages ved hjelp av sentrum og størrelsen på blobene
            point = keypoint.pt
            size = 10 + 3*int(keypoint.size)
            box = (int(point[0])- (size/2),int(point[1]) - (size/2), size, size)
            boxes.append(box)
        return boxes
\end{minted}
\caption{Kode som viser hvordan nøkkelpunkter fra blob-deteksjonen blir gjort om til bokser som kan brukes til trackingen.}
\label{code:keypointsToBoxes}
\end{listing}

Når alle nøkkelpunkter er gjort om til bokser, må det identifiseres hvilke blobs som allerede trackes. 
Det kan gjøres ved å sjekke om midten av noen av blobene ligger innenfor boksen til en blob som allerede trackes. 
Hvis den gjør det skal ikke bloben trackes på nytt. 
Funksjon\newline \python{removeTrackedBlobs(keypoints, boxes)} i \autoref{code:removeTrackedBlobs} viser hvordan dette gjøres.
I denne funksjonen blir nøkkelpunktene til blobene som allerede trackes, fjernet fra listen. 
Nøkkelpunktene til de resterende blobene blir returnert.

\begin{listing}[!htb]
\begin{minted}{python}
    def removeTrackedBlobs(keypoints, boxes):
        #newKeypoints: liste med nye blobs, diff: for allerede tracka blobs
        newKeypoints = []
        diff = []
        try:
        #Sjekker om blobs er nye. Legges til i tilhørende liste
            for points in keypoints:
                x = points.pt[0]
                y = points.pt[1]
                for box in boxes:
                    xb,yb,wb,hb = box
                    if xb<x and x<xb+wb and yb<y and y<yb+hb:
                        diff.append(points)
            for point in keypoints:
                if point not in diff:
                    newKeypoints.append(point)
        except:
            pass
        #returnerer alle nye blobs
        return newKeypoints
\end{minted}
\caption{Kodeeksempelet viser hvordan det sjekkes om blobs er nye eller om de trackes fra et tidligere bilde.}
\label{code:removeTrackedBlobs}
\end{listing}

\textit{OpenCV} har flere funksjoner som hjelper til med denne trackingen, og har flere forskjellige metoder for tracking som varierer i nøyaktighet og tidsbruk.
For å opprette en tracker av en av disse trackertypene kalles funksjonen \python{createTrackerByName(trackerType)}, der \python{trackerType} er hvilken type tracker som blir brukt. En rask og god tracker som er valgt i dette prosjektet er \textit{CSRT-trackeren}. 
Etter trackeren er konstruert, må den initialiseres.
Dette gjøres ved å si hvilket bilde bloben er i og koordinater til en boks som skal omslutte bloben. 
Siden det kan være flere fugler i et bilde, er det nødvendig å kunne lage flere trackere, og lagre disse sammen.

\textit{OpenCV} har en multitrackerklasse, men denne er ikke optimal, da det ikke kan slettes trackere fra et multitracker-objekt. 
Derfor er en ny klasse implementert med de ønskede egenskapene. 
Denne klassen er kalt \python{NewTracker} og en del av implementeringen av denne klassen kan sees i \autoref{code:newTracker}. 
Klassen består av to lister og en rekke medlemsfunksjoner. 
Listen \python{trackers} består av enkle trackere, og listen \python{trackerFail} består av heltall som forteller i hvor mange bilder etter hverandre trackeren har mislyktes med å følge en blob. 
Når blob-deteksjons-algoritmen har kjørt, legges det til trackere til alle nye blobs med medlemsfunksjonen \python{add()}.
I tillegg har \python{NewTracker}-klassen medlemsfunksjonene \python{pop()} og \python{update()} \cite{GitHub}. 
Funksjonen \python{pop()} fjerner trackere, og \python{update()} oppdaterer alle trackere. 
I tillegg til å oppdatere trackerene, vil \python{update()}-funksjonen inkrementere listeelementene i \python{trackerFail} som hører til trackere som mislyktes med sporingen sin.
Dersom trackeren klarte å følge bloben blir tilhørende \python{trackerFail}-element satt til $0$. 
Alle trackere som ikke har detektert bloben sin i fem bilder på rad, vil fjernes fra multitrackeren. Funksjonen \python{update()} returnerer også en liste med koordinatene til alle trackere.


\begin{listing}[!htb]
\begin{minted}{python}
    class NewTracker():                     
        def __init__(self):
        #objektet består av to lister
        #En med trackerne, og en som vet hvor lenge en tracker har vært ugyldig
            self.trackers = []             
            self.trackerFail = []
    
        def add(self, trackerObj):
        #når en ny tracker legges til, legges den inn i self.trackers
        #i tillegg får den et element i trackerFail-lista,
        #som viser at den har vært ugyldig i 0 bilder
            self.trackers.append(trackerObj)
            self.trackerFail.append(0)
\end{minted}
\caption{Konstruktøren og add-funksjonen til multitrackeren.}
\label{code:newTracker}
\end{listing}


\subsubsection{Logging}\label{sec:impl:programvare:logging}
Det er ikke alltid systemet fungerer som det skal.
Derfor er det svært nyttig å ha en loggfil der all viktig informasjon om programmets aktiviteter lagres. 
Både operasjonene programmet gjør vellykket og det programmet ikke har fått til, blir lagt til i loggfilen. 
Begge heldelser er nyttig informasjon. 
Alt som blir lagt til i logging-fila, blir også merket med hvor viktig informasjonen er, for eksempel \python{INFO} eller \python{CRITICAL}. 
Denne vil også fungere som en reserveløsning dersom man mister nettverksforbindelsen.

\subsection{Værstasjon}\label{sec:impl:vaer}

Værstasjonen vil måle temperatur, trykk og luftfuktighet, i tillegg til vindhastighet, vindretning og nedbørsmengde. 
All dataen sendes via et egendesignet kretskort til prosesseringsenheten, som prosesserer dataen og sender det videre til databasen for så å bli gjort tilgjengelig på nettsiden.


\subsubsection{Trykk, temperatur, luftfuktighet}\label{sec:impl:vaer:ttl}

Alle luftdata hentes fra en BME280-sensor fra Bosch \cite{bme280}, som måler både temperatur, trykk og fuktighet i lufta. 
Kommunikasjonen mellom sensoren og Raspberry Pi-en foregår over en I2C-protokoll, en synkron, seriell bussprotokoll \cite{i2c}. 
Denne bussprotokollen tillater toveiskommunikasjon, og bruker 7-bits adresser. 
I2C tillater dermed opp til 128 enheter ved bruk av kun to ledninger. 
Figur \ref{fig:luftsensor_krets-} under viser kretstegning av luftsensoren.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{implementering/vaer/luftsensor_krets.png}
    \caption{Kretsskjema for luftsensoren.}
    \label{fig:luftsensor_krets-}
\end{figure}

De eneste eksterne komponentene BME280-sensoren, $U2$, trenger rundt seg, er en kondensator ved hver av de to spenningsinngangene \texttt{VDD} og \texttt{VDDIO}, $C3$ og $C4$, og en opptrekksmotstand (eng: pull-up resistor) til databussen \texttt{SDA} og en til klokkebussen \texttt{SCL}, henholdsvis $R4$ og $R5$. \\
Figur \ref{fig:luftsensor_3d} under viser 3D-modell av kretskortet.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{implementering/vaer/luftsensor_3d.png}
    \caption{3D-modell av luftsensoren.}
    \label{fig:luftsensor_3d}
\end{figure}

For å lese data fra sensoren finnes det et eget bibliotek som heter \textit{bme280lib} \cite{bme280lib}, som gjør det enkelt å lese data fra sensoren med en \python{sample()}-funksjon. 
Biblioteket \textit{smbus2} \cite{smbus2} brukes også for å opprette en I2C bussforbindelse. 
Programmet som leser dataen, ligger i \textit{bme280.py} filen under \textit{Weather-station} i GitHuben\cite{GitHub}.

\subsubsection{Anemometer}\label{sec:impl:vaer:anemometer}

\begin{wrapfigure}{r}{0.38\textwidth}
    \centering
    \includegraphics[width=0.35\textwidth]{implementering/vaer/anemometer.png}
    \caption{Figur av anemometeret som brukes.}
    \label{fig:anemometer}
\end{wrapfigure}

Anemometeret som brukes, vist i figur \ref{fig:anemometer}, måler vindhastigheten. 
Sensoren bruker en \textit{reed-bryter}, en mekanisk bryter som lukkes av et magnetisk felt. 
For hver omdreining beveger en magnet seg forbi denne bryteren, slik at den lukkes i et kort øyeblikk. 
Ved å telle antall ganger bryteren lukkes per tidsenhet, kan rotasjonshastigheten til vindmåleren beregnes. 
Dette kan igjen brukes til å regne ut vindhastigheten. 
I følge databladet\cite{weather} vil bryteren lukkes én gang i sekundet ved en vindhastighet på $\SI{2.4}{\kilo\meter\per\hour}$, som tilsvarer $\SI{0.67}{\meter\per\second}$.

For å telle antall omdreininger, brukes klassen \python{Button} fra biblioteket \textit{gpiozero} for å registrere når reed-bryteren lukkes. Programme til anemometeret, ligger i \textit{WeatherStationMain.py} under \textit{Weather-station} i GitHuben\cite{GitHub}.

%https://www.argentdata.com/files/80422_datasheet.pdf

\newpage
\subsubsection{Vindretning}\label{sec:impl:vaer:vindret}

\begin{wrapfigure}{r}{0.38\textwidth}
    \centering
    \includegraphics[width=0.35\textwidth]{implementering/vaer/vindretning.png}
    \caption{Figur av vindretningssensoren som brukes.}
    \label{fig:vindretning}
\end{wrapfigure}

Retningsmåleren, vist i figur \ref{fig:vindretning}, består av 8 reed-brytere plassert i en sirkel slik at det er $45\degree$ mellom hver bryter \cite{weather}. 
En værhane med en påmontert magnet vil rette seg mot vinden, og vil lukke enten en bryter eller to nabobrytere om gangen, avhengig av posisjonen.
Dette resulterer i totalt 16 posisjonskombinasjoner, og dermed en nøyaktighet på $\pm 11.25\degree$. 
De 16 ulike kombinasjonene vil koble signalpinen til 16 ulike motstandsverdier, som gjør at vindretingen kan leses som et analogt signal ved hjelp av en spenningsdeler for å finne retningen. 
Da Pi-en ikke har en innebygd analog/digital-omformer (heretter kalt ADC), sendes det analoge signalet via en MCP3221A5T ADC til Pi-en.
Spenningsdelingen og ADC-en, samt programmet som henter ut data fra ADC-en, er diskutert nærmere i \autoref{sec:impl:vaer:kretskort}.

Når det analoge signalet fra sensoren er lest av prosesseringsenheten, brukes et \textit{dictionary}\footnote{En type uordnet liste der man lagrer data under en nøkkel(key)\cite{dictionaries}.} for å koble de ulike verdiene til ulike vinkler.
Dette ble kalibrert ved manuelt å rotere sensoren, og lese av verdien ved ulike vinkler. 
Programmet returnerer så en retningsvinkel i grader, hvor 0\degree er definert som nord. 
Programmet ligger i \textit{windDirection.py} under \textit{Weather-station} i GitHuben\cite{GitHub}.

%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.5\textwidth]{implementering/vindretning.png}
%    \caption{Figur av vindretningssensoren som brukes.}
%    \label{fig:vindretning}
%\end{figure}

\subsubsection{Nedbør}\label{sec:impl:vaer:regn}

\begin{wrapfigure}{r}{0.38\textwidth}
    \centering
    \includegraphics[width=0.35\textwidth]{implementering/vaer/rain_gauge.jpg}
    \caption{Figur av nedbørsmåleren som brukes.}
    \label{fig:impl:vaer:regn}
\end{wrapfigure}

Nedbørsmåleren består av to selvtømmende beholdere, som fylles opp og tipper over etter en gitt mengde nedbør. 
Når den ene beholderen er fylt opp og tipper over, renner vannet ut under sensoren, og den nye beholderen blir plassert under vanninntaket. 
Denne sensoren bruker også en reed-bryter, som trigges av en magnet hver gang en beholder tipper over. 
Ved å telle antall ganger bryteren lukkes og multiplisere dette med vannmengden før en beholder tipper, beregnes mengden nedbør som har falt i et gitt tidsrom. 
Mengden nedbør før en beholder tipper, er i databladet\cite{weather} definert til å være $0,2794$mm. Figur \ref{fig:impl:vaer:regn} viser bilde av nedbørsmåleren.

Tilsvarende anemometeret i \autoref{sec:impl:vaer:anemometer}, brukes også klassen \python{Button} fra biblioteket \textit{gpiozero} her for å detektere hver gang reed-bryteren lukkes. Programme ligger i \textit{weatherStationMain.py} under \textit{Weather-station} i GitHuben\cite{GitHub}.

\newpage
\subsubsection{Kretskort til sensorer}\label{sec:impl:vaer:kretskort}

For å enkelt kunne koble alle sensorene til Pi-en, brukes et egenutviklet kretskort.
Figur \ref{fig:sensorkretskort_krets} viser kretstegning til dette kretskortet.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{implementering/vaer/sensorkretskort_krets.png}
    \caption{Kretsskjema for kretskortet til sensorene.}
    \label{fig:sensorkretskort_krets}
\end{figure}


\begin{wrapfigure}{r}{0.38\textwidth}
    \centering
    \includegraphics[width=0.35\textwidth]{implementering/vaer/sensorkretskort_3d.png}
    \caption{3D-modell av kretskortet til sensorene.}
    \label{fig:sensorkretskort_3d}
\end{wrapfigure}

I tillegg til kontakter for å koble til sensorene, inneholder kretskortet en motstand, $R1$, til spenningsdeleren til vindretningsensoren. 
Videre inneholder kretskortet en ADC, $U1$, for å digitalisere signalet fra spenningsdeleren. 
ADC-en som brukes er en MCP3221A5T fra Microchip\cite{adc}. 
Omformeren tar inn et analogt signal, og kommuniserer direkte over I2C-bussprotokollen med Pi-en. 
Opptrekksmotstander, $R2$ og $R3$, brukes også her for å trekke \texttt{SDA} og \texttt{SCL} bussene høye, og to kondensatorer, $C1$ og $C2$, brukes for å filtrere bort ulike støyfrekvenser fra spenningsforsyningen. 
Figur \ref{fig:sensorkretskort_3d} viser en 3D-modell av kretskortet.

For å lese data fra MCP3221A5T ADC-en, brukes igjen biblioteket \textit{smbus2}\cite{smbus2}. 
Funksjonen \\ \python{read_byte_data(address, byte)} fra dette biblioteket, brukes for enkelt å lese data fra ADC-en over I2C-protokollen. 

Luftsensoren og ADC-en til vindretningssensoren kommuniserer med prosesseringsenheten direkte via samme I2C-buss. 
GPIO pin 2 og 3 brukes til henholdsvis databussen, \texttt{SDA}, og klokkebussen, \texttt{SCL}, til kommunikasjon over I2C-protokollen.
Anemometeret og nedbørssensoren kobles til henholdsvis GPIO pin 5 og 6 på Pi-en via kretskortet. 
Alle senorene bruker 3.3 volt spenning fra Pi-en.
Oppkoblingen er vist i \autoref{fig:kretsskjema_pi}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{implementering/vaer/kretsskjema_pi.png}
    \caption{Figuren viser oppkoblingen av værstasjonen til prosesseringsenheten.}
    \label{fig:kretsskjema_pi}
\end{figure}


\subsection{Nettside og database}\label{sec:impl:nettside}

Nettsiden har som mål å vise informasjonen som hentes inn av både fugletelleren og værstasjonen. 
Til dette brukes Python-rammeverket \textit{Dash} \cite{dash}. 
Dash er laget spesifikt for å prosessere og vise store mengder data i enkle grafer og oppgi disse på en statisk nettside. 
Se \autoref{fig:nettside:nettside}  for en eksempelnettside.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.9\textwidth]{implementering/nettside/nettside.png}
    \caption{Eksempelnettside til Jolyu.}
    \label{fig:nettside:nettside}
\end{figure}

Når nettsiden lastes inn, vil data fra databasen hentes fra nettet. 
Deretter prosesserer og filtrerer programmet dataen slik at det ikke bare er punktdata, men data basert på timer, dager, måneder og så videre. 
Dette krever effektive sorterings- og filtreringsalgoritmer, da dataen kan inneholde flere tusen punkter.
Noen av disse algoritmene er allerede implementert i ferdige biblioteker, mens andre må implementeres med vanlige funksjoner i Python. 
Flyten på nettsiden kan sees i flytdiagrammet i \autoref{fig:nettside:flytdiagram}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.5\textwidth]{implementering/nettside/JolyuNettsideOverordnetFlyt.pdf}
    \caption{Flytdiagrammet til nettsiden.}
    \label{fig:nettside:flytdiagram}
\end{figure}

Store deler av nettsiden fungerer på samme måte. 
Det kjøres en funksjon om det velges data i en graf, eller om nettsiden bes hente et nytt datasett. 
Dette registreres ved hjelp av en \textit{callback}, se \autoref{sec:impl:nettside:callback}.
Det vil derfor kun gis noen eksempler i avsnittene under, og koden til hele nettsiden kan leses på Jolyu sin GitHub\footnote{Direkte til nettside repository: \url{https://github.com/jolyu/nettside}}.

\subsubsection{Rammeverket}\label{sec:impl:nettside:rammeverk}

\textit{Dash} \cite{dash} er en kombinasjon og mellomledd mellom det pythonbaserte nettside-rammeverket for statiske nettsider, \textit{flask}, \cite{flask} og javascript biblioteket \textit{plotly} \cite{plotly} for å plotte data.

\textit{Dash} fungerer ved å bruke spesialobjekter fra \textit{plotly} som \textit{graph} og \textit{buttons}, og vanlige \html{html} objekter som \html{div} og \html{header}. 
Disse defineres i en liste med nøkkelverdier for variabler. Se \autoref{code:impl:nettside:dash1} som er tatt fra dokumentasjonen til \textit{Dash}. 

\begin{listing}[!htb]
\begin{minted}{python}
    import dash_core_components as dcc
    dcc.Dropdown(
        options=[
            {'label': 'New York City', 'value': 'NYC'},
            {'label': 'Montréal', 'value': 'MTL'}
        ],
        value='MTL'
    )
\end{minted}
\caption{Eksempel på hvordan implementere en enkel \textit{dropdown} (valgliste) i \textit{Dash}.}
\label{code:impl:nettside:dash1}
\end{listing}

\textit{Flask} er i utgangspunktet laget for statiske nettsider, der data er ferdig laget når nettsiden lastes inn. 
Det \textit{Dash} implementerer i \textit{flask}, er hvordan lage mer interaktive nettsider ved å legge til callbacks, funksjoner som kjører når du for eksempel trykker på en knapp eller endrer på zoom i en graf.
Det er slik store deler av nettsiden er bygget opp, ved at brukeren endrer på hvilke objekter som er valgt eller drar i en slider. 
Da vil det skje en callback som vil kunne endre på data og deretter oppdatere nettsiden. 



\subsubsection{Callbacks}\label{sec:impl:nettside:callback}

Nettsiden bruker stort sett callbacks for å endre på hvilke data som skal vises i de forskjellige grafene. 
%Dersom det blir tatt utgangspunkt i en forenklet figur av nettsiden vist i \autoref{fig:impl:nettside:forenkletNettside} og \ref{fig:impl:nettside:callback}, kan vi s
En callback er en type funksjon som reagerer når noe endres på eller en hendelse, som et knappetrykk. 
Deretter kjøres funksjonen for så å returnere data til et annet objekt. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{implementering/nettside/NettsideEnkel.pdf}
    \caption{Forenklet figur av nettsiden med hva elementer i grå bokser og navnet til elementet i rød teks, eventuelt med pil til elementet.}
    \label{fig:impl:nettside:forenkletNettside}
\end{figure}

Alle de grå elementene i \autoref{fig:impl:nettside:forenkletNettside} brukes til å gjøre en callback for ulike ting. 
De røde navnene er navnene på elementene som vil kjøre en callback.
Figur \ref{fig:impl:nettside:callback} viser hvordan callbacks flytter data rundt om på siden. 
Ta \python{fetchDbButton} som eksempel. 
Når knappen klikkes på, vil data skrives til alle stedene pilene peker til. 
Der de blå boksene med tekst er forskjellige atributter i hvert av elementene.


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.7\textwidth]{implementering/nettside/callbacks.png}
    \caption{Callback-diagram for nettsiden. De grønne nodene er et callback som utløses av det som kommer inn, og skriver til der pilene peker.}
    \label{fig:impl:nettside:callback}
\end{figure}

\newpage
Når \python{dbDates} oppdateres, trigger det to forskjellige callbacks. 
Den ene vil oppdatere \html{href} i \python{downloadBut} med nedlastning av det aktive datasettet, og den andre vil oppdatere hovedgrafen.
I \autoref{code:impl:nettside:callback} vises det hvordan callback-en for oppdatering av nedlastning fungerer. 

\begin{listing}[!htb]
\begin{minted}{python} 
    @app.callback(
        Output("downloadBut", "href"),
        [Input("dbDates", "data"),]
    )
    def UpdateDownloadButton(dates):
        """ Oppdaterer nedlasningsknappen med den nyeste dataen """
        # Sjekk om variabelen dates ikke er tom
        if dates == None:
            dates = GetInitialDates(ref, initialDays)
        else:
            dates = pd.to_datetime(dates)
        # Hent det aktive datasettet
        df = QueryDF(ref, dates)
        # Gjør om datasettet til data som kan lagres i en fil 
        # uten ulovelige bokstaver
        csvString = df.to_csv(encoding="utf-8")
        csvString = "data:text/csv;charset=utf-8," + urlParse.quote(csvString)
        return csvString
\end{minted}
\caption{Callback som oppdaterer nedlastningen av det aktive datasettet, basert på en oppdatering av aktive datoer i datasettet.}
\label{code:impl:nettside:callback}
\end{listing}

En callback initialiseres ved å si hva som skal være \python{Input} (en type \textit{trigger}) og hvor data skal plasseres når callback er ferdig med en \python{Output}. 
I dette tilfellet vil callback-en kjøres når \python{dbDates} endres. 
Denne inneholder første og siste dato i det aktive datasettet. 
Denne oppdateres av \python{fetchDbButton}, som henter datasettet fra databasen, som vist i \autoref{fig:impl:nettside:callback}. 
Den vil deretter lage en \python{string} med alle aktive datasett og plassere det som en link i atributten \html{href}.

Dette er bare en av callback-ene på nettsiden, for å se hele nettsiden kan en gå til Jolyu sin GitHub \cite{GitHub}.

\subsubsection{Sortering og filtrering}\label{sec:impl:nettside:sortering}

Sortering og filtrering kan gjøres på mange måter. 
På nettsiden brukes en avansert liste, en \python{dataframe} fra biblioteket \textit{pandas} \cite{dataframe}. 
Den har sine egne sorteringsalgoritmer som brukes aktivt på nettsiden. 

Den mest intense sorteringen er å samle all punktdataen til måneder (eller uker/dager). 
Da det kan være flere hundretusen datapunkter, og sorteringsalgoritmen må derfor være effektiv. 
Derfor brukes de innebygde funksjonene for akkurat dette formålet. 
I \autoref{code:impl:nettside:sortering1} kan det ses hvordan funksjonen \python{dataframe.resample(<timegroup>).sum()} i \python{DataToMonths(df)} brukes for å summere alle punkter i en måned, sammen til et enkelt punkt.

\begin{listing}[!htb]
\begin{minted}{python}
    def DataToMonths(df):
        """ Sorter data til måneder """
        df = df.resample('M').sum()
        return df
\end{minted}
\caption{Funksjoner som sorterer punkter på måneder og uker.}
\label{code:impl:nettside:sortering1}
\end{listing}

For å hente ut en liten del av datasettet brukes noen selvlagde funksjoner. 
I \\ \python{FilterData(df, startDate, endDate)} sendes en dataframe, startdato og sluttdato inn. 
Funksjonen vil finne de aktuelle datapunktene som er innenfor dette tidsintervallet og returnere en ny dataframe.

\begin{listing}[!htb]
\begin{minted}{python}
    def FilterData(df, startDate, endDate):
        """ Filtrer ut en del av datasettet """
        dff = df.loc[(df.index > startDate) & (df.index < endDate)]
        return dff
\end{minted}
\caption{Funksjon som henter ut et område innenfor to gitte datoer.}
\label{code:impl:nettside:sortering2}
\end{listing}


\subsubsection{Database}\label{sec:impl:nettside:database}

Kommunikasjon mellom Pi-en og nettsiden går via Google sin \textit{Realtime Firebase}-database. 
Denne kommunikasjonen foregår over Wi-Fi. 
Dataene Pi-en samler inn blir sendt til databasen som \textit{dictionaries} og lagret der på et NoSQL-format. 
En NoSQL-database er ikke like streng med struktur i forhold til en SQL-database, så den takler bedre endringer i strukturen til dataene. 
Firebase har Python-bibliotek for overføring mellom prosesseringsenhet og databasen, og fra databasen til nettsiden. 
Måten dette fungerer på blir abstrahert bort av ferdigutviklet software, som ligger i \textit{firebase\_admin} biblioteket.

All data i databasen har en nøkkel som er et tidsstempel\footnote{Unix tidsstempel (eng: Unix Timestamp) - Tid i sekunder fra 1. januar 1970. Brukes mye i datateknologi.} som angir når det ble lastet opp i databasen. 
Når data blir hentet fra databasen kan det sorteres etter nøklene eller de forskjellige dataene som ligger innenfor hovednøkkelen.
Et eksempel på en slik database er en enkel \html{json}\footnote{Et filformat bassert på \textit{dictionaries}.}-liste, som sett i \autoref{code:impl:nettside:json}.

\begin{listing}[!htb]
    \begin{minted}{json}
    {
        "1609455600": {
            "time": 1609455600.0, 
            "birds": 8, 
            "Temperature": 7.5, 
            "Wind": 0.1
        }  
    }
    \end{minted}
    \caption{Enkel data i json. Dette er også en bra måte å visualisere en \textit{dictionary}.}
    \label{code:impl:nettside:json}
\end{listing}

\newpage
\subsection{Strukturelt}\label{sec:impl:struktur}

Systemet vil monteres til en aluminiumsstolpe, som vist i \autoref{fig:3dsystem}. 
Her er boksen festet til stangen $\SI{1}{\meter}$ over bakken med modulene til værstasjonen festet øverst, $\SI{1.5}{\meter}$ over bakken. 
Stativet vil plasseres på et flatt underlag, og vil forankres i bakken med vaier eller veies ned for å forhindre velting som følge av for eksempel sterk vind.
Kameraet festes inne i boksen som vist i \autoref{fig:boks}.
Boksen er tiltenkt å printes i PETG-plast på en 3D-printer, med relativt tykke vegger for å være vanntett. 

En 3D-modell av boksen er vist i figur \ref{fig:boks}, og en 3D-modell av strukturen til hele systemet er vist i figur \ref{fig:3dsystem}.

\begin{figure}[!htbp]
\centering
\begin{minipage}[c]{0.45\textwidth}
\centering
    \includegraphics[width=0.9\textwidth]{implementering/Boks_render.png}
    \caption{3D-modell av boksen som skal holde kamera og prosesseringsenhet.}
    \label{fig:boks}
    
\end{minipage}
\begin{minipage}[c]{0.45\textwidth}
\centering
    \includegraphics[width=0.9\textwidth]{implementering/stativ_render.png}
    \caption{3D-modell av hele systemets fysiske realisering.}
    \label{fig:3dsystem}
\end{minipage}
\end{figure}